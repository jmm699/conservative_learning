\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,graphicx,gensymb,cite,multicol,mathtools}
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\graphicspath{}
\allowdisplaybreaks


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\nn}{\mathbf{\hat{n}}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\xx}{\mathbf{\hat{x}}}
\newcommand{\yy}{\mathbf{\hat{y}}}
\newcommand{\zz}{\mathbf{\hat{z}}}
\newcommand{\rr}{\mathbf{\hat{r}}}
\newcommand{\ee}{\mathbf{\hat{e}}}
\newcommand{\rrr}{\mathbf{r}}
\newcommand{\ppp}{\mathbf{p}}
\newcommand{\xxx}{\mathbf{x}}
\newcommand{\limn}{\lim_{n \rightarrow \infty}}
\newcommand{\limk}{\lim_{k \rightarrow \infty}}
\newcommand{\nnot}{\sim \!}
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

%for EM:
\newcommand{\EE}{\mathbf{E}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\VA}{\mathbf{A}}

%for QM:
\newcommand{\ev[1]}{\left\langle #1 \right\rangle}
\newcommand{\intii}{\int_{-\infty}^\infty}
\newcommand{\intoi}{\int_0^\infty}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\ang}[3]{\,^{#1} {#2}_{#3}}
%\newcommand{\bra[1]}{\left\langle #1 \right\right|}
%\newcommand{\ket[1]}{\left\left| #1 \right\rangle}
%\newcommand{\braket}[2]{\left\langle #1  #2 \right\rangle}
\usepackage{braket}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\Sp}{\mathbf{S}}
\newcommand{\J}{\mathbf{J}}

\renewcommand{\ij}{i \rightarrow j} 

\begin{document}

\title{Using Node Biases with Conservative Learning}
\author{John Mastroberti}
 
\maketitle

As a modification to the conservative learning algorithm outlined here\cite{cl_update}, we want to add a bias term to the nodes:
\begin{align}
y_j^k & = \sum_{i \rightarrow j} x_i^k w_{i \rightarrow j} - b_j
\end{align}
We can accomplish this by introducing a special input node which is always set to 1.
This node is then connected to all of the hidden and output nodes, 
and if we give this special node the index $s$, we can define
\begin{align}
w_{s \rightarrow j} & = -b_j
\end{align}
so that we can retain the formula 
\begin{align}
y_j^k & = \sum_{i \rightarrow j} x_i^k w_{i \rightarrow j}
\end{align}
Using this formalism leaves the update algorithm largely unchanged.
The important differences are as follows.

While equation (1.15),
\begin{align}
\forall (i \in H, k \in K): & \,\,\, \gamma_i^k = df_i^k \sum_{i \rightarrow j} w_{i \rightarrow j} \gamma_j^k
\end{align}
remains unchanged, since $s \not\in H$, equation (1.16),
\begin{align}
\forall (\ij \in E): \,\,\, \Delta w_{\ij} + \delta w_{\ij}
& = \sum_{k \in K} \tilde{x}_i^k \gamma_{j}^k \\
& = \tilde{x}_i \cdot \gamma_j
\end{align}
now encompasses the special case
\begin{align}
\forall (j \in H \cup O): \,\,\, -\Delta b_j + -\delta b_j 
& = \tilde{x}_s \cdot \gamma_j
\end{align}
which means that equation (1.18) now reads
\begin{align}
\forall (j \in H_-, k \in K): \,\,\, \delta y_j^k
& = \sum_{\ij} \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij})
+ \tilde{x}_s^k (\tilde{x}_s \cdot \gamma_j + \Delta b_j) \\
& = \sum_{\ij} \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij})
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j
\end{align}
since $\tilde{x}_s^k = 1$.
Furthermore, the forward propagation equation (1.19) gets the slight modification
\begin{align}
\forall (j \not\in H_-, k \in K): \,\,\, \delta y_j^k & = 
\sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}) ) 
+ \tilde{x}_s^k (\tilde{x}_s \cdot \gamma_j + \Delta b_j) \\
& = \sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}) ) 
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j
\end{align}
since there are no $\delta y_j^k$'s or $df_i^k$'s for $i \in I$, including our special input node $s$.

Finally, the initialization formulas need to be modified slightly;
most notably, an extra row is added to $\tilde{X}$ for our special node $s$,
and $\delta W$ and $\Delta W$ get an extra row for the biases:
\begin{align}
\forall (j \in O, k \in K): \,\,\, \epsilon_j^k & = 
\sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k \delta w_{\ij}) 
- \delta b_j \\
\mathcal{E}(n)_{kj} & = \epsilon_j^k - \sum_{\ij} df_i^k w_{\ij} \delta y(n)_i^k \\
\tilde{X} & = \left( \begin{array}{ccccc}
        \tilde{x}_1^1 & \tilde{x}_1^2 & \cdots & \tilde{x}_1^k & \cdots \\
        \tilde{x}_2^1 & \tilde{x}_2^2 & \cdots & \tilde{x}_2^k & \cdots \\
        \vdots & \vdots & \ddots & \vdots & \ddots \\
        1 & 1 & \cdots & 1 & \cdots \\
        \end{array}
        \right) \\
\delta W(n) & = \left( \begin{array}{ccccc}
        \delta w_{1 \rightarrow 1} & \delta w_{1 \rightarrow 2} & \cdots & \delta w_{1 \rightarrow j} & \cdots \\
        \delta w_{2 \rightarrow 1} & \delta w_{2 \rightarrow 2} & \cdots & \delta w_{2 \rightarrow j} & \cdots \\
        \vdots & \vdots & \ddots & \vdots & \ddots \\
        -\delta b_1 & -\delta b_2 & \cdots & -\delta b_j & \cdots \\
        \end{array}
        \right) \\
\Delta W(n) & = \left( \begin{array}{ccccc}
        \Delta w_{1 \rightarrow 1} & \Delta w_{1 \rightarrow 2} & \cdots & \Delta w_{1 \rightarrow j} & \cdots \\
        \Delta w_{2 \rightarrow 1} & \Delta w_{2 \rightarrow 2} & \cdots & \Delta w_{2 \rightarrow j} & \cdots \\
        \vdots & \vdots & \ddots & \vdots & \ddots \\
        -\Delta b_1 & -\Delta b_2 & \cdots & -\Delta b_j & \cdots \\
        \end{array}
        \right)
\end{align}
With these slight modifications, and maintaining $\Gamma(n)_{kj} = \gamma(n)_j^k$ from equation (1.29),
the form of equation (1.30) remains unchanged:
\begin{align}
\Gamma(n) & = (\tilde{X}^\top \tilde{X})^{-1} (\tilde{X}^\top \Delta W + \mathcal{E}(n))
\end{align}

Thus, the steps for an inner loop now read
\begin{enumerate}
\item
Initialize $\gamma$'s at the output nodes:
\begin{align}
\Gamma(n) & = (\tilde{X}^\top \tilde{X})^{-1} (\tilde{X}^\top \Delta W + \mathcal{E}(n)) \tag{17}
\end{align}

\item
Back-propagate $\gamma$'s down to all the hidden nodes:
\begin{align}
\forall (i \in H, k \in K): & \,\,\, \gamma_i^k = df_i^k \sum_{i \rightarrow j} w_{i \rightarrow j} \gamma_j^k \tag{4}
\end{align}

\item
Initialize $\delta y$'s at nodes adjacent to the input nodes:
\begin{align}
\forall (j \in H_-, k \in K): \,\,\, \delta y_j^k
& = \sum_{\ij} \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij})
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j \tag{9}
\end{align}

\item
Forward propagate $\delta y$'s up to the nodes adjacent to the output nodes:
\begin{align}
\forall (j \not\in H_-, k \in K): \,\,\, \delta y_j^k & = 
\sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}) ) 
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j \tag{11}
\end{align}

\end{enumerate}
We still obtain the weight changes from (1.33),
\begin{align}
\forall (\ij \in E): \,\,\, \delta w_{\ij} & = \tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}
\end{align}
and we obtain the bias changes from
\begin{align}
\forall (j \in H \cup O) : \delta b_j & = -\sum_{\kappa \in K} \gamma_j^\kappa - \Delta b_j
\end{align}




\bibliography{refs}{}
\bibliographystyle{plain}

\end{document}
