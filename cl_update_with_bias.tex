\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,graphicx,gensymb,cite,multicol,mathtools}
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}
\graphicspath{}
\allowdisplaybreaks


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\nn}{\mathbf{\hat{n}}}
\newcommand{\pp}{\mathbf{p}}
\newcommand{\uu}{\mathbf{u}}
\newcommand{\vv}{\mathbf{v}}
\newcommand{\ww}{\mathbf{w}}
\newcommand{\xx}{\mathbf{\hat{x}}}
\newcommand{\yy}{\mathbf{\hat{y}}}
\newcommand{\zz}{\mathbf{\hat{z}}}
\newcommand{\rr}{\mathbf{\hat{r}}}
\newcommand{\ee}{\mathbf{\hat{e}}}
\newcommand{\rrr}{\mathbf{r}}
\newcommand{\ppp}{\mathbf{p}}
\newcommand{\xxx}{\mathbf{x}}
\newcommand{\limn}{\lim_{n \rightarrow \infty}}
\newcommand{\limk}{\lim_{k \rightarrow \infty}}
\newcommand{\nnot}{\sim \!}
\let\oldemptyset\emptyset
\let\emptyset\varnothing
\renewcommand{\Re}{\mathrm{Re}}
\renewcommand{\Im}{\mathrm{Im}}

%for EM:
\newcommand{\EE}{\mathbf{E}}
\newcommand{\DD}{\mathbf{D}}
\newcommand{\BB}{\mathbf{B}}
\newcommand{\VA}{\mathbf{A}}

%for QM:
\newcommand{\ev[1]}{\left\langle #1 \right\rangle}
\newcommand{\intii}{\int_{-\infty}^\infty}
\newcommand{\intoi}{\int_0^\infty}
\newcommand{\HH}{\mathbb{H}}
\newcommand{\ang}[3]{\,^{#1} {#2}_{#3}}
%\newcommand{\bra[1]}{\left\langle #1 \right\right|}
%\newcommand{\ket[1]}{\left\left| #1 \right\rangle}
%\newcommand{\braket}[2]{\left\langle #1  #2 \right\rangle}
\usepackage{braket}
\newcommand{\tr}{\mathrm{Tr}}
\newcommand{\Sp}{\mathbf{S}}
\newcommand{\J}{\mathbf{J}}

\renewcommand{\ij}{i \rightarrow j} 

\begin{document}

\title{Using Node Biases with Conservative Learning}
\author{John Mastroberti}
 
\maketitle

As a modification to the conservative learning algorithm outlined here\cite{cl_update}, we want to add a bias term to the nodes:
\begin{align}
y_j^k & = \sum_{i \rightarrow j} x_i^k w_{i \rightarrow j} - b_j
\end{align}
We can accomplish this by introducing a special input node which is always set to 1.
This node is then connected to all of the hidden and output nodes, 
and if we give this special node the index $s$, we can define
\begin{align}
w_{s \rightarrow j} & = -b_j
\end{align}
so that we can retain the formula 
\begin{align}
y_j^k & = \sum_{i \rightarrow j} x_i^k w_{i \rightarrow j}
\end{align}
Using this formalism leaves the update algorithm largely unchanged.
The important differences are as follows.

While equation (1.15),
\begin{align}
\forall (i \in H, k \in K): & \,\,\, \gamma_i^k = df_i^k \sum_{i \rightarrow j} w_{i \rightarrow j} \gamma_j^k
\end{align}
remains unchanged, since $s \not\in H$, equation (1.16),
\begin{align}
\forall (\ij \in E): \,\,\, \Delta w_{\ij} + \delta w_{\ij}
& = \sum_{k \in K} \tilde{x}_i^k \gamma_{j}^k \\
& = \tilde{x}_i \cdot \gamma_j
\end{align}
now encompasses the special case
\begin{align}
\forall (j \in H \cup O): \,\,\, -\Delta b_j + -\delta b_j 
& = \tilde{x}_s \cdot \gamma_j
\end{align}
which means that equation (1.18) now reads
\begin{align}
\forall (j \in H_-, k \in K): \,\,\, \delta y_j^k
& = \sum_{\ij} \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij})
+ \tilde{x}_s^k (\tilde{x}_s \cdot \gamma_j + \Delta b_j) \\
& = \sum_{\ij} \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij})
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j
\end{align}
since $\tilde{x}_s^k = 1$.
Furthermore, the forward propagation equation (1.19) gets the slight modification
\begin{align}
\forall (j \not\in H_-, k \in K): \,\,\, \delta y_j^k & = 
\sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}) ) 
+ \tilde{x}_s^k (\tilde{x}_s \cdot \gamma_j + \Delta b_j) \\
& = \sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k (\tilde{x}_i \cdot \gamma_j - \Delta w_{\ij}) ) 
+ \sum_{\kappa \in K} \gamma_j^\kappa + \Delta b_j
\end{align}
since there are no $\delta y_j^k$'s or $df_i^k$'s for $i \in I$, including our special input node $s$.

Finally, the initialization formulas need to be modified slightly;
most notably, an extra row is added to $\tilde{X}$ for our special node $s$,
and $\delta W$ and $\Delta W$ get an extra row for the biases:
\begin{align}
\forall (j \in O, k \in K): \,\,\, \epsilon_j^k & = 
\sum_{\ij} (df_i^k \delta y_i^k w_{\ij} + \tilde{x}_i^k \delta w_{\ij}) \\
- \delta b_j
\end{align}



\bibliography{refs}{}
\bibliographystyle{plain}

\end{document}
